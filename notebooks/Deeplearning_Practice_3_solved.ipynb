{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "YR5MrQjQbfHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "UwdML8fSb9Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CIFAR-10 dataset from Hugging Face https://huggingface.co/datasets/uoft-cs/cifar10\n",
        "dataset_cifar10 = load_dataset('cifar10')"
      ],
      "metadata": {
        "id": "0CVbCgBLEX2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_cifar10"
      ],
      "metadata": {
        "id": "jV9kpnIcEM7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the training and test sets\n",
        "train_set = dataset_cifar10['train']\n",
        "test_set = dataset_cifar10['test']\n",
        "\n",
        "# Convert the data into numpy arrays for consistency with your original code\n",
        "train_set_x_orig = np.stack([np.array(item['img']) for item in train_set])\n",
        "train_set_y = np.array([item['label'] for item in train_set])\n",
        "\n",
        "test_set_x_orig = np.stack([np.array(item['img']) for item in test_set])\n",
        "test_set_y = np.array([item['label'] for item in test_set])\n",
        "\n",
        "# Example of a picture\n",
        "index = 24\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print(\"y = \" + str(train_set_y[index]))"
      ],
      "metadata": {
        "id": "pq2EkG_9btHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_x_orig.shape"
      ],
      "metadata": {
        "id": "qkcLYRc6btMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set_y)"
      ],
      "metadata": {
        "id": "_ePgnbwbbfSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: data transformation Reshape the training and test data ((50000, 32, 32, 3)) sets so that images are flattened into single vectors ((5000,32*32*3))\n",
        "\n",
        "# Reshape the training and test examples\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
        "\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
      ],
      "metadata": {
        "id": "50D1c0f6IQyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_x = train_set_x_flatten / 255.\n",
        "test_set_x = test_set_x_flatten / 255."
      ],
      "metadata": {
        "id": "_duRic20HCpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the data to keep only label 0 and 1 for the classification problem\n",
        "train_set_x = train_set_x[:, train_set_y <= 1]\n",
        "train_set_y = train_set_y[train_set_y <= 1]"
      ],
      "metadata": {
        "id": "LjzQLRvAHTKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same for test\n",
        "test_set_x = test_set_x[:, test_set_y <= 1]\n",
        "test_set_y = test_set_y[test_set_y <= 1]"
      ],
      "metadata": {
        "id": "hINRvhHuK3hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_x.shape"
      ],
      "metadata": {
        "id": "6yqvK4BhLIuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# form train_set_x keep 10% for train and 10% for validation\n",
        "\n",
        "new_size = int(train_set_x.shape[1] * 0.1)\n",
        "# 10% for validation\n",
        "validation_set_x = train_set_x[:, new_size:2*new_size]\n",
        "validation_set_y = train_set_y[new_size:2*new_size]\n",
        "\n",
        "# 10% for training\n",
        "train_set_x = train_set_x[:, :new_size]\n",
        "train_set_y = train_set_y[:new_size]"
      ],
      "metadata": {
        "id": "AAQeGGgPLQ51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_set_x.shape\n"
      ],
      "metadata": {
        "id": "Nc8UqrsmFcnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_x.shape"
      ],
      "metadata": {
        "id": "1Oc8lyrSLfWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_y.shape"
      ],
      "metadata": {
        "id": "uzTSTEfjOxb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "Am6XPkbHMeo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init your weights and variables\n",
        "w_it=np.random.rand(train_set_x.shape[0],1)*0.01\n",
        "b_it=0\n",
        "cost_values = []\n",
        "accuracy_values = []\n",
        "cost_values_validation = []\n",
        "accuracy_values_validation = []\n",
        "alpha = 0.01"
      ],
      "metadata": {
        "id": "xQ95QwqLQ1zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(w, b, X, Y):\n",
        "  Z = np.dot(w.T, X) + b\n",
        "  A = sigmoid(Z)\n",
        "  m=X.shape[1]\n",
        "  cost = -1/m * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
        "  return A, cost"
      ],
      "metadata": {
        "id": "-m0bOMlOLnZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(w, b, X, Y, A):\n",
        "  m = X.shape[1]  # number of examples\n",
        "  delta_w = np.dot(X, (A - Y).T) / m\n",
        "  delta_b = np.sum(A - Y) / m\n",
        "  new_w = w - alpha * delta_w\n",
        "  new_b = b - alpha * delta_b\n",
        "  return new_w, new_b"
      ],
      "metadata": {
        "id": "FXJ0FGdBLntj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forward(w_it, b_it, train_set_x, train_set_y)"
      ],
      "metadata": {
        "id": "o2nvwcxqM-S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run a loop of 100 steps using backward and forward and plot the cost function\n",
        "for i in range(1000):\n",
        "  predictions, cost = forward(w_it, b_it, train_set_x, train_set_y)\n",
        "  predictions_validation, cost_validation = forward(w_it, b_it, validation_set_x, validation_set_y)\n",
        "  cost_values.append(cost)\n",
        "  cost_values_validation.append(cost_validation)\n",
        "  accuracy_values.append( np.mean((predictions >= 0.5) == train_set_y))\n",
        "  accuracy_values_validation.append( np.mean((predictions_validation >= 0.5) == validation_set_y))\n",
        "  w_it, b_it = backward(w_it, b_it, train_set_x, train_set_y, predictions)"
      ],
      "metadata": {
        "id": "XqDMh5ciLv2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot cost_values and cost_values_validation\n",
        "plt.plot(cost_values, label='Training Cost')\n",
        "plt.plot(cost_values_validation, label='Validation Cost')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plot accuracy_values and accuracy_values_validation\n",
        "plt.plot(accuracy_values, label='Training Accuracy')\n",
        "plt.plot(accuracy_values_validation, label='Validation Accuracy')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_JVMGA7VRLBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: compute accuracy on test dataset\n",
        "\n",
        "# Convert probabilities to class labels (0 or 1)\n",
        "predictions = forward(w_it, b_it, test_set_x, test_set_y)[0]\n",
        "predictions_labels = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(predictions_labels == test_set_y)\n",
        "print(f\"Accuracy on test dataset: {accuracy}\")"
      ],
      "metadata": {
        "id": "Mu0sp-wARbLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reshape train_set_y and validation_set_y to be (1, M) for consistency\n",
        "train_set_y = train_set_y.reshape(1, train_set_y.shape[0])\n",
        "validation_set_y = validation_set_y.reshape(1, validation_set_y.shape[0])"
      ],
      "metadata": {
        "id": "JOiH-opcnvCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----- helpers -----\n",
        "def sigmoid(Z):\n",
        "    return 1.0 / (1.0 + np.exp(-Z))\n",
        "\n",
        "def sigmoid_derivative(A):\n",
        "    # derivative wrt Z when A = sigmoid(Z)\n",
        "    return A * (1.0 - A)\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return (Z > 0).astype(int)\n",
        "\n",
        "# ----- model size & hyperparams -----\n",
        "n_x = train_set_x.shape[0]      # input dimension (features)\n",
        "eps = 1e-12                     # for numerical stability in logs\n",
        "\n",
        "# ----- parameter init -----\n",
        "n_1 = 10                        # <- set number of hidden units here\n",
        "rng = np.random.default_rng(0)\n",
        "W1 = rng.normal(0, 0.01, size=(n_1, n_x))\n",
        "b1 = np.zeros((n_1, 1))\n",
        "W2 = rng.normal(0, 0.01, size=(1, n_1))\n",
        "b2 = 0.0\n",
        "alpha = 0.1\n",
        "num_iters = 1000\n",
        "\n",
        "cost_values = []\n",
        "accuracy_values = []\n",
        "cost_values_validation = []\n",
        "accuracy_values_validation = []\n",
        "alpha = 0.01\n",
        "\n",
        "# ===== layer-wise forward =====\n",
        "def forward_hidden(W1, b1, X):\n",
        "    Z1 = W1 @ X + b1        # (n1, m)\n",
        "    A1 = relu(Z1)        # (n1, m)\n",
        "    return Z1, A1\n",
        "\n",
        "def forward_output(W2, b2, A1, Y):\n",
        "    Z2 = W2 @ A1 + b2       # (1, m)\n",
        "    A2 = sigmoid(Z2)        # (1, m)\n",
        "    m = Y.shape[1]\n",
        "    cost = -np.sum(Y*np.log(A2 + eps) + (1 - Y)*np.log(1 - A2 + eps)) / m\n",
        "    return Z2, A2, cost\n",
        "\n",
        "# ===== layer-wise backward =====\n",
        "def backward_output(W2, b2, A1, Y, A2):\n",
        "    # Using sigmoid + BCE â†’ dZ2 = A2 - Y\n",
        "    m = Y.shape[1]\n",
        "    dZ2 = A2 - Y                                     # (1, m)\n",
        "    dW2 = (dZ2 @ A1.T) / m                           # (1, n1)\n",
        "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m     # (1, 1)\n",
        "    dA1 = W2.T @ dZ2                                 # (n1, m)\n",
        "    return dW2, db2, dA1\n",
        "\n",
        "def backward_hidden(W1, b1, X, A1, dA1):\n",
        "    # dZ1 = dA1 * sigma'(Z1); but we have A1, so use derivative via A1\n",
        "    m = X.shape[1]\n",
        "    dZ1 = dA1 * relu_derivative(A1)               # (n1, m)\n",
        "    dW1 = (dZ1 @ X.T) / m                             # (n1, n_x)\n",
        "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m     # (n1, 1)\n",
        "    return dW1, db1\n",
        "\n",
        "# ===== convenience wrappers =====\n",
        "def forward(W1, b1, W2, b2, X, Y):\n",
        "    Z1, A1 = forward_hidden(W1, b1, X)\n",
        "    Z2, A2, cost = forward_output(W2, b2, A1, Y)\n",
        "    cache = {\"X\": X, \"Y\": Y, \"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
        "    return A2, cost, cache\n",
        "\n",
        "def backward(W1, b1, W2, b2, cache, alpha):\n",
        "    X, Y, A1, A2 = cache[\"X\"], cache[\"Y\"], cache[\"A1\"], cache[\"A2\"]\n",
        "    dW2, db2, dA1 = backward_output(W2, b2, A1, Y, A2)\n",
        "    dW1, db1 = backward_hidden(W1, b1, X, A1, dA1)\n",
        "\n",
        "    # gradient step\n",
        "    W2_new = W2 - alpha * dW2\n",
        "    b2_new = b2 - alpha * db2\n",
        "    W1_new = W1 - alpha * dW1\n",
        "    b1_new = b1 - alpha * db1\n",
        "    return W1_new, b1_new, W2_new, b2_new, dW1, db1, dA1\n",
        "\n",
        "# ===== training loop =====\n",
        "for i in range(num_iters):\n",
        "    # train forward\n",
        "    A2_train, cost_train, cache_train = forward(W1, b1, W2, b2, train_set_x, train_set_y)\n",
        "    cost_values.append(cost_train)\n",
        "\n",
        "    # validation forward\n",
        "    _, cost_val, _ = forward(W1, b1, W2, b2, validation_set_x, validation_set_y)\n",
        "    cost_values_validation.append(cost_val)\n",
        "\n",
        "    # accuracies\n",
        "    acc_train = np.mean((A2_train >= 0.5) == (train_set_y == 1))\n",
        "    accuracy_values.append(acc_train)\n",
        "\n",
        "    A2_val, _, _ = forward(W1, b1, W2, b2, validation_set_x, validation_set_y)\n",
        "    acc_val = np.mean((A2_val >= 0.5) == (validation_set_y == 1))\n",
        "    accuracy_values_validation.append(acc_val)\n",
        "\n",
        "    # backward + update\n",
        "    W1, b1, W2, b2, _, _, _ = backward(W1, b1, W2, b2, cache_train, alpha)\n",
        "\n",
        "# ===== plots =====\n",
        "plt.plot(cost_values, label='Training Cost')\n",
        "plt.plot(cost_values_validation, label='Validation Cost')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(accuracy_values, label='Training Accuracy')\n",
        "plt.plot(accuracy_values_validation, label='Validation Accuracy')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "of5hoygXlljt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(accuracy_values_validation)"
      ],
      "metadata": {
        "id": "SnJpDc6gllm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try RELU\n",
        "# mini-batch\n",
        "# gradien checking"
      ],
      "metadata": {
        "id": "6xTBXNvDllqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_minibatches(X, Y, batch_size):\n",
        "    m = X.shape[1]\n",
        "    minibatches = []\n",
        "    for start in range(0, m, batch_size):\n",
        "        end = min(start + batch_size, m)\n",
        "        minibatches.append((X[:, start:end], Y[:, start:end]))\n",
        "    return minibatches\n"
      ],
      "metadata": {
        "id": "OkXRNFNvlltm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_1 = 10                       # <- set number of hidden units here\n",
        "alpha = 0.1\n",
        "num_epoch = 200\n",
        "\n",
        "# init your weights and variables\n",
        "rng = np.random.default_rng(0)\n",
        "W1 = rng.normal(0, 0.01, size=(n_1, n_x))\n",
        "b1 = np.zeros((n_1, 1))\n",
        "W2 = rng.normal(0, 0.01, size=(1, n_1))\n",
        "b2 = 0.0\n",
        "cost_values = []\n",
        "accuracy_values = []\n",
        "cost_values_validation = []\n",
        "accuracy_values_validation = []\n",
        "alpha = 0.01\n",
        "batch_size=int(train_set_x.shape[1]/5)\n",
        "# Precompute once before training\n",
        "train_minibatches = build_minibatches(train_set_x, train_set_y, batch_size)\n",
        "accuracy_values_validation = []\n",
        "for epoch in range(num_epoch):\n",
        "    for batch in train_minibatches:\n",
        "        X_batch, Y_batch = batch\n",
        "\n",
        "        # train forward\n",
        "        A2_train, cost_train, cache_train = forward(W1, b1, W2, b2, X_batch, Y_batch)\n",
        "        cost_values.append(cost_train)\n",
        "\n",
        "        # validation forward\n",
        "        _, cost_val, _ = forward(W1, b1, W2, b2, validation_set_x, validation_set_y)\n",
        "        cost_values_validation.append(cost_val)\n",
        "\n",
        "        # accuracies\n",
        "        acc_train = np.mean((A2_train >= 0.5) == (Y_batch == 1))\n",
        "        accuracy_values.append(acc_train)\n",
        "\n",
        "        A2_val, _, _ = forward(W1, b1, W2, b2, validation_set_x, validation_set_y)\n",
        "        acc_val = np.mean((A2_val >= 0.5) == (validation_set_y == 1))\n",
        "        accuracy_values_validation.append(acc_val)\n",
        "\n",
        "        # backward + update\n",
        "        W1, b1, W2, b2, _, _, _ = backward(W1, b1, W2, b2, cache_train, alpha)\n",
        "\n",
        "# ===== plots =====\n",
        "plt.plot(cost_values, label='Training Cost')\n",
        "plt.plot(cost_values_validation, label='Validation Cost')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(accuracy_values, label='Training Accuracy')\n",
        "plt.plot(accuracy_values_validation, label='Validation Accuracy')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5j_xbmc6woxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(0)\n",
        "W1 = rng.normal(0, 0.1, size=(n_1, n_x))\n",
        "b1 = np.zeros((n_1, 1))\n",
        "W2 = rng.normal(0, 0.1, size=(1, n_1))\n",
        "b2 = 0.0\n",
        "epsilon = 0.000001\n",
        "\n",
        "def cost_function(W1, b1, W2, b2, X, Y):\n",
        "    _, cost, _ = forward(W1, b1, W2, b2, X, Y)\n",
        "    return cost\n",
        "\n",
        "def grad_numeric(W1, b1, W2, b2, X, Y, i, j):\n",
        "  W1_plus_epsilon = W1.copy()\n",
        "  W1_plus_epsilon[i][j] = W1[i][j] + epsilon\n",
        "  W1_minus_epsilon = W1.copy()\n",
        "  W1_minus_epsilon[i][j] = W1[i][j] - epsilon\n",
        "  cost_plus_epsilon = cost_function(W1_plus_epsilon, b1, W2, b2, X, Y)\n",
        "  cost_minus_epsilon = cost_function(W1_minus_epsilon, b1, W2, b2, X, Y)\n",
        "  grad_numeric = (cost_plus_epsilon - cost_minus_epsilon) / (2 * epsilon)\n",
        "  return grad_numeric"
      ],
      "metadata": {
        "id": "kwlMvjC1y8zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 3\n",
        "j = 3\n",
        "\n",
        "grad_numeric_value= grad_numeric(W1, b1, W2, b2, train_set_x, train_set_y, i, j)\n",
        "\n",
        "print(\"numeric grad_numeric_value  = \", grad_numeric_value)"
      ],
      "metadata": {
        "id": "EH8aKwr_WD0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, cache = forward(W1, b1, W2, b2, train_set_x, train_set_y)\n",
        "W1_tmp, b1_tmp, W2_tmp, b2_tmp, dW1, db1, dA1 = backward(W1, b1, W2, b2, cache, alpha=0.0)\n",
        "\n",
        "\n",
        "print(\"real gradient     =\", dW1[i, j])"
      ],
      "metadata": {
        "id": "bMCEVtaFWdUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop on all W1 weights and compute the norm of diff\n",
        "grad_numeric_value = []\n",
        "grad_real_value = []\n",
        "for i in range(W1.shape[0]):\n",
        "  for j in range(W1.shape[1]):\n",
        "    grad_numeric_value.append(grad_numeric(W1, b1, W2, b2, train_set_x, train_set_y, i, j))\n",
        "    grad_real_value.append(dW1[i,j])\n",
        "# compute the norme\n",
        "grad_numeric_value = np.array(grad_numeric_value)\n",
        "grad_real_value = np.array(grad_real_value)\n",
        "norm_diff = np.linalg.norm(grad_numeric_value - grad_real_value)\n",
        "\n",
        "# divide the norm_diff by both normes\n",
        "norm_diff = norm_diff / (np.linalg.norm(grad_numeric_value) + np.linalg.norm(grad_real_value))\n",
        "print(\"norm_diff = \", norm_diff)\n",
        "\n"
      ],
      "metadata": {
        "id": "pAHBBVE29Ca0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}